Matthew Corfiatis - COMP261 Assignment 5

Marker Instructions:
Hello marker, you can run the main method in the main.java.Assignment5 package inside src to open the GUI used to test Huffman and LempelZiv.
I've used JUnit tests to run/test the rest of the assignment which you can find in the test.java package inside src.
At the time of submission, all tests are passing so you should also expect all tests to pass. (You can skip the performance tests as they might take a few minutes and will always pass)
Some tests may fail if the data files are not in the data directory and I have included some extra data files that I used while testing my code.

Questions:
Q1:
I played around with different test data and found that when all of the patterns were a valid match in the text, or did not exist at all, the brute force algorithm was faster.
The brute force algorithm was also faster on smaller patterns for smaller text due to the overhead of setting up KMP.
KMP became faster when I added shuffled data in to the patterns to search for.

For most small uses of string search I do not think KMP is worth the trouble. It will usually end up slower and it is harder to implement.
And it turns out that the Java implementation of String::indexOf doesn't use KMP or any "fast" algorithm to do its job, it just uses the brute force approach.

Performance results:
BRUTE FORCE:
Time taken for Brute Force: 2722 ms for 6105 TINY patterns | 0.45 ms per pattern.
Time taken for Brute Force: 8044 ms for 17196 SMALL patterns | 0.47 ms per pattern.
Time taken for Brute Force: 5336 ms for 11306 MEDIUM patterns | 0.47 ms per pattern.
Time taken for Brute Force: 8261 ms for 17263 LARGE patterns | 0.48 ms per pattern.
Time taken for Brute Force: 1 ms for 3 HUGE patterns | 0.33 ms per pattern.
Total time taken for Brute Force was 24.36 seconds.

KMP:
Time taken for KMP: 2005 ms for 6103 TINY patterns | 0.33 ms per pattern.
Time taken for KMP: 6451 ms for 17205 SMALL patterns | 0.37 ms per pattern.
Time taken for KMP: 3984 ms for 11304 MEDIUM patterns | 0.35 ms per pattern.
Time taken for KMP: 7564 ms for 17272 LARGE patterns | 0.44 ms per pattern.
Time taken for KMP: 2 ms for 3 HUGE patterns | 0.67 ms per pattern.
Total time taken for KMP was 20.01 seconds.


Q2:
War and Peace size after Huffman encode: 1848598 bytes
Character encoding values:
'\n' =  000111
'\r' =  000101
' '  =  001
'a'  =  0111
'b'  =  0000011
'c'  =  010001
'd'  =  01001
'e'  =  111
'f'  =  011001
'g'  =  011011
'h'  =  1101
'i'  =  1011
','  =  0000001
'-'  =  011010011
'.'  =  0001101
'!'  =  0001100001
'j'  =  00000100110
'k'  =  1000001
'l'  =  10001
'm'  =  010000
'n'  =  1010
'o'  =  1001
'p'  =  0000000
'q'  =  00000100011
'r'  =  00001
's'  =  1100
't'  =  0101
'u'  =  000100
'v'  =  0110101
'w'  =  011000
'x'  =  0001100000
'y'  =  100001
'z'  =  00000100010
'"'  =  00000101
'?'  =  0110100001
'A'  =  100000001
'B'  =  0001100111
'C'  =  10000001001
'D'  =  00000100111
'E'  =  10000001000
'F'  =  00011001001
'G'  =  000001000011
'H'  =  0001100101
'I'  =  011010001
'''  =  000110001
'R'  =  00000100101
'S'  =  1000000001
'T'  =  011010010
'('  =  0000010000001
')'  =  100000000001
'*'  =  00000100100001
'0'  =  000001001000101
'1'  =  00000100100011
'2'  =  000001001000100
'3'  =  1000000000001001
'4'  =  10000000000010101
'5'  =  1000000000001011
'6'  =  1000000000001000
'7'  =  10000000000000001
'8'  =  10000000000011
'9'  =  10000000000000011
':'  =  000110010001
';'  =  000001001001
'J'  =  00000100100000
'K'  =  000001000010
'L'  =  0000010000000
'M'  =  0110100000
'N'  =  0001100110
'O'  =  10000000001
'P'  =  100000011
'Q'  =  10000000000000000
'U'  =  10000000000001
'V'  =  000110010000
'W'  =  1000000101
'ê'  =  100000000000101001
'X'  =  10000000000000010
'Y'  =  000001000001
'Z'  =  100000000000001
'à'  =  1000000000001010001
'/'  =  100000000000101000001
'='  =  100000000000101000000
'é'  =  1000000000001010000100
'ä'  =  1000000000001010000101
'\uFEFF'  =  100000000000101000011


Q3:
Best result was Taisho with a compression ratio of 2.36x
Data that has a large number of common characters will usually compress better because the common characters can be assigned a very small binary prefix.
Another thing that affects the compression is how many distinct characters the data has. Data with less distinct characters will compress better than files with lots of unique characters.


Q4:
When the LZ window is made larger, the compression takes much longer but produces a better compressed result.

Performance results:
LZ Compress Time: 122ms			Decompress Time: 97ms		Window Size: 8		    Size: 16,168,250
LZ Compress Time: 131ms			Decompress Time: 65ms		Window Size: 16		    Size: 1,4251,129
LZ Compress Time: 155ms			Decompress Time: 56ms		Window Size: 32		    Size: 1,2532,486
LZ Compress Time: 225ms			Decompress Time: 50ms		Window Size: 64		    Size: 1,0952,764
LZ Compress Time: 349ms			Decompress Time: 50ms		Window Size: 128		Size:  9,937,466
LZ Compress Time: 549ms			Decompress Time: 47ms		Window Size: 256		Size:  8,964,194
LZ Compress Time: 910ms			Decompress Time: 42ms		Window Size: 512		Size:  7,910,582
LZ Compress Time: 1645ms		Decompress Time: 41ms		Window Size: 1024		Size:  7,137,948


Q5:
War and Peace file compression.
Size of LZ from original:        9865729 characters
Size of Huffman encode then LZ: 14657551 characters

The LZ compressed and huffman encoded data is much larger than just using the LZ compression.
This is because when you compress data, it increases the entropy of the data so that each new byte encodes lots of new information. This is what the huffman encoding is doing.
When LZ compression is run on this again, it cant compress the data much further and the overhead ends up outweighing the benefits making the data larger.
The data cannot be compressed much more because it has already been compressed to near the fundamental losses data compression limit from Shannon's source coding theorem.


Q6:
ngram trained on war_and_peace.txt
Probability of 'a' after "a tak": 0.0010559662 n: 3

Maori text: Turn your face to the sun and the shadows fall behind you, translation
Log2 Probability: -143.39763

English text: Hurihia to aroaro ki te ra tukuna to atarangi kia taka ki muri i a koe
Log2 Probability: -353.18784

These log probabilities are so much different because the ngram model was trained on an entirely english dataset.
Each language has its own patterns like common phrases, letters or sequences of characters like "ing" or "ion" in english.
When the ngram model is trained, it builds a model which can fairly accurately predict these language patterns, so when the model is run
The model has high probabilities for these english patterns of text, so the English whakatauki has a much higher probability than the Maori one which lacks most of the english patterns.


Q7:
184 bits for War and Peace
79 bits for Grammar of New Zealand


Q8:
Encode:
```
let len = |A| + |B|
let binaryLength = Log2(len + 1)
let binary = encode |A| using binaryLength bits
return binary + A + B
```

Decode
```
let x = |Z|
while (x > 0)
    let l = Log2(x + 1)
    if(l + x <= |Z|)
        let aLen = binary decode Z from index 0 to l
        let A = take Z from l to aLen + l
        let B = take Z from aLen + l to |Z|

        return (A, B)
    end if
    decrement x
end while
no result/return null
```

This relies on the logic that when you encode |A| in binary if you use Log2(len + 1) bits, |Z| = Log2(len + 1) + len where len is |A| + |B|
This can then be decoded by initializing len to |Z| and decrementing it until the result is less than or equal to |Z|
and then we know how many bits the length header is, and can then get A, and the rest of the string is B

Working version of this code in main.java.binarystrings.BinaryStrings
Code can be run using JUnit test in test.java.binarystrings.BinaryTest