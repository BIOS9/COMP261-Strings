Brute force algorithm takes O(mn) time where m is the length of the data and n is the length of the pattern
KMP takes O(m + n) where m is the length of the data and n is the n is the length of the pattern. m time is taken to search the string, n time is taken to generate the pattern table

in KMP the index in the data never moves backwards.

lps
0 1 2 3 4 5 6 7 8 9  10 11 12
a b c d x a b d y a  b  c  a
0 0 0 0 0 1 2 0 0 1  2  3  1

put position + 1 in the lps table, which is the point to start the next comparison from because it can skip position
When character is mismatched, the prefix index jumps to the position notated in the previous slot, not 0.

To build lps table:

```
let i = 1
let j = 0

let pattern = string to search for
let table = array with length of pattern string, initialized to empty or 0

initialize first slot in table to 0 if not already done

while (i < length of table)
    if (pattern[i] equals pattern[j])
        table[i] = j + 1
        i = i + 1
        j = j + 1
    else
        if (j equals 0)
            table[i] = 0;
            i = i + 1
        else
            j = table[j - 1]
        end if
    end if
end while
```

to search using the table

```
let data = data string in which to search for pattern

let i = 0 // Index in the data to search, never goes backwards
let j = 0 // Index in the pattern to search for

let pattern = string to search for
let table = pattern prefix table

while (i < (length of data - length of pattern) + 1)
    if (data[i] equals pattern[j])
        i = i + 1
        j = j + 1

        if (j equals length of pattern)
            match found at i - j
            either return result here,
            or j = table[j - 1] // j cannot return to start in case pattern can occur intersecting with itself e.g pattern "aaa" might
        end if
    else
        if (j equals 0)
            i = i + 1
        else
            j = table[j - 1]
        end if
    end if
end while

return no match if not already returned
```


Performance results:
BRUTE FORCE:
Time taken for Brute Force: 2722 ms for 6105 TINY patterns | 0.45 ms per pattern.
Time taken for Brute Force: 8044 ms for 17196 SMALL patterns | 0.47 ms per pattern.
Time taken for Brute Force: 5336 ms for 11306 MEDIUM patterns | 0.47 ms per pattern.
Time taken for Brute Force: 8261 ms for 17263 LARGE patterns | 0.48 ms per pattern.
Time taken for Brute Force: 1 ms for 3 HUGE patterns | 0.33 ms per pattern.
Total time taken for Brute Force was 24.36 seconds.

KMP:
Time taken for KMP: 2005 ms for 6103 TINY patterns | 0.33 ms per pattern.
Time taken for KMP: 6451 ms for 17205 SMALL patterns | 0.37 ms per pattern.
Time taken for KMP: 3984 ms for 11304 MEDIUM patterns | 0.35 ms per pattern.
Time taken for KMP: 7564 ms for 17272 LARGE patterns | 0.44 ms per pattern.
Time taken for KMP: 2 ms for 3 HUGE patterns | 0.67 ms per pattern.
Total time taken for KMP was 20.01 seconds.

I played around with different test data and found that when all of the patterns were a valid match in the text, or did not exist at all, the brute force algorithm was faster.
The brute force algorithm was also faster on smaller patterns for smaller text due to the overhead of setting up KMP.
KMP became faster when I added shuffled data in to the patterns to search for.

For most small uses of string search I do not think KMP is worth the trouble. It will usually end up slower and it is harder to implement.
And it turns out that the Java implementation of String::indexOf doesn't use KMP or any "fast" algorithm to do its job, it just uses the brute force approach.

Huffman results:
War and peace:
input length:  3258246 bytes
output length: 1848598 bytes
original and decoded texts match.
Compression ratio: 1.76

Taisho:
input length:  3649944 bytes
output length: 1542656 bytes
original and decoded texts match.
Compression ratio: 2.36

Pi:
input length:  1010003 bytes
output length: 443632 bytes
original and decoded texts match.
Compression ratio: 2.27

Best result was Taisho with a compression ratio of 2.36

LZ77:
Can use []| to delimit tuples in text as they do not occur
every tuple has a character to put on the end
[offset, length, next symbol]
e.g [0,0,j] for if first character was j

Outputs a string of the mentioned tuples
Moves through the text one character at a time
Drags a sliding windows behind the cursor
Expands a lookahead buffer from the cursor/The string it tries to match in the sliding window

Pseudocode:
```
let cursor = 0
let windowSize = 100

while (cursor < text.length - 1)
    look for longest prefix of text[cursor .. text.length - 1] in text[max(cursor-windowSize, 0) .. cursor - 1] // max is used to select either from start of text (0) or start of window

    if found, add [offset, length, text[cursor+length]] to output
    else add [0, 0, text[cursor]] to output
    end if

    advance cursor by length + 1
end while
```

Probably just use brute force java indexOf to find match

More pseudocode:

```
let cursor = 0
let windowSize = 100

while (cursor < text.length - 1)
    let length = 0
    let prevMatch = 0

    loop
        let match = stringMatch(text[cursor.. cursor+length],
            text[(cursor<windowSize)?0:cursor-windowSize .. cursor - 1])

        if match succeeded then:
            prevMatch = match
            length = length + 1
        else
            output ([a value for prevMatch, length, text[cursor+length]])
            cursor = cursor + length + 1
            break
        end if
    end loop
end while
```

Performance results:
LZ Compress Time: 122ms			Decompress Time: 97ms		Window Size: 8		    Size: 16,168,250
LZ Compress Time: 131ms			Decompress Time: 65ms		Window Size: 16		    Size: 1,4251,129
LZ Compress Time: 155ms			Decompress Time: 56ms		Window Size: 32		    Size: 1,2532,486
LZ Compress Time: 225ms			Decompress Time: 50ms		Window Size: 64		    Size: 1,0952,764
LZ Compress Time: 349ms			Decompress Time: 50ms		Window Size: 128		Size:  9,937,466
LZ Compress Time: 549ms			Decompress Time: 47ms		Window Size: 256		Size:  8,964,194
LZ Compress Time: 910ms			Decompress Time: 42ms		Window Size: 512		Size:  7,910,582
LZ Compress Time: 1645ms		Decompress Time: 41ms		Window Size: 1024		Size:  7,137,948





Questions:

Q1:
I played around with different test data and found that when all of the patterns were a valid match in the text, or did not exist at all, the brute force algorithm was faster.
The brute force algorithm was also faster on smaller patterns for smaller text due to the overhead of setting up KMP.
KMP became faster when I added shuffled data in to the patterns to search for.

For most small uses of string search I do not think KMP is worth the trouble. It will usually end up slower and it is harder to implement.
And it turns out that the Java implementation of String::indexOf doesn't use KMP or any "fast" algorithm to do its job, it just uses the brute force approach.

Performance results:
BRUTE FORCE:
Time taken for Brute Force: 2722 ms for 6105 TINY patterns | 0.45 ms per pattern.
Time taken for Brute Force: 8044 ms for 17196 SMALL patterns | 0.47 ms per pattern.
Time taken for Brute Force: 5336 ms for 11306 MEDIUM patterns | 0.47 ms per pattern.
Time taken for Brute Force: 8261 ms for 17263 LARGE patterns | 0.48 ms per pattern.
Time taken for Brute Force: 1 ms for 3 HUGE patterns | 0.33 ms per pattern.
Total time taken for Brute Force was 24.36 seconds.

KMP:
Time taken for KMP: 2005 ms for 6103 TINY patterns | 0.33 ms per pattern.
Time taken for KMP: 6451 ms for 17205 SMALL patterns | 0.37 ms per pattern.
Time taken for KMP: 3984 ms for 11304 MEDIUM patterns | 0.35 ms per pattern.
Time taken for KMP: 7564 ms for 17272 LARGE patterns | 0.44 ms per pattern.
Time taken for KMP: 2 ms for 3 HUGE patterns | 0.67 ms per pattern.
Total time taken for KMP was 20.01 seconds.



Q2:
War and Peace size after Huffman encode: 1848598 bytes
Character encoding values:
'\n' =  000111
'\r' =  000101
' '  =  001
'a'  =  0111
'b'  =  0000011
'c'  =  010001
'd'  =  01001
'e'  =  111
'f'  =  011001
'g'  =  011011
'h'  =  1101
'i'  =  1011
','  =  0000001
'-'  =  011010011
'.'  =  0001101
'!'  =  0001100001
'j'  =  00000100110
'k'  =  1000001
'l'  =  10001
'm'  =  010000
'n'  =  1010
'o'  =  1001
'p'  =  0000000
'q'  =  00000100011
'r'  =  00001
's'  =  1100
't'  =  0101
'u'  =  000100
'v'  =  0110101
'w'  =  011000
'x'  =  0001100000
'y'  =  100001
'z'  =  00000100010
'"'  =  00000101
'?'  =  0110100001
'A'  =  100000001
'B'  =  0001100111
'C'  =  10000001001
'D'  =  00000100111
'E'  =  10000001000
'F'  =  00011001001
'G'  =  000001000011
'H'  =  0001100101
'I'  =  011010001
'''  =  000110001
'R'  =  00000100101
'S'  =  1000000001
'T'  =  011010010
'('  =  0000010000001
')'  =  100000000001
'*'  =  00000100100001
'0'  =  000001001000101
'1'  =  00000100100011
'2'  =  000001001000100
'3'  =  1000000000001001
'4'  =  10000000000010101
'5'  =  1000000000001011
'6'  =  1000000000001000
'7'  =  10000000000000001
'8'  =  10000000000011
'9'  =  10000000000000011
':'  =  000110010001
';'  =  000001001001
'J'  =  00000100100000
'K'  =  000001000010
'L'  =  0000010000000
'M'  =  0110100000
'N'  =  0001100110
'O'  =  10000000001
'P'  =  100000011
'Q'  =  10000000000000000
'U'  =  10000000000001
'V'  =  000110010000
'W'  =  1000000101
'ê'  =  100000000000101001
'X'  =  10000000000000010
'Y'  =  000001000001
'Z'  =  100000000000001
'à'  =  1000000000001010001
'/'  =  100000000000101000001
'='  =  100000000000101000000
'é'  =  1000000000001010000100
'ä'  =  1000000000001010000101
'\uFEFF'  =  100000000000101000011


Q3:
Best result was Taisho with a compression ratio of 2.36x
Data that has a large number of common characters will usually compress better because the common characters can be assigned a very small binary prefix.
Another thing that affects the compression is how many distinct characters the data has. Data with less distinct characters will compress better than files with lots of unique characters.


Q4:
When the LZ window is made larger, the compression takes much longer but produces a better compressed result.

Performance results:
LZ Compress Time: 122ms			Decompress Time: 97ms		Window Size: 8		    Size: 16,168,250
LZ Compress Time: 131ms			Decompress Time: 65ms		Window Size: 16		    Size: 1,4251,129
LZ Compress Time: 155ms			Decompress Time: 56ms		Window Size: 32		    Size: 1,2532,486
LZ Compress Time: 225ms			Decompress Time: 50ms		Window Size: 64		    Size: 1,0952,764
LZ Compress Time: 349ms			Decompress Time: 50ms		Window Size: 128		Size:  9,937,466
LZ Compress Time: 549ms			Decompress Time: 47ms		Window Size: 256		Size:  8,964,194
LZ Compress Time: 910ms			Decompress Time: 42ms		Window Size: 512		Size:  7,910,582
LZ Compress Time: 1645ms		Decompress Time: 41ms		Window Size: 1024		Size:  7,137,948


Q5:
War and Peace file compression.
Size of LZ from original:        9865729 characters
Size of Huffman encode then LZ: 14657551 characters

The LZ compressed and huffman encoded data is much larger than just using the LZ compression.
This is because when you compress data, it increases the entropy of the data so that each new byte encodes lots of new information. This is what the huffman encoding is doing.
When LZ compression is run on this again, it cant compress the data much further and the overhead ends up outweighing the benefits making the data larger.
The data cannot be compressed much more because it has already been compressed to near the fundamental losses data compression limit from Shannon's source coding theorem.


Q6:
ngram trained on war_and_peace.txt
Probability of 'a' after "a tak": 0.0010559662 n: 3

Maori text: Turn your face to the sun and the shadows fall behind you, translation
Log2 Probability: -143.39763

English text: Hurihia to aroaro ki te ra tukuna to atarangi kia taka ki muri i a koe
Log2 Probability: -353.18784

These log probabilities are so much different because the ngram model was trained on an entirely english dataset.
Each language has its own patterns like common phrases, letters or sequences of characters like "ing" or "ion" in english.
When the ngram model is trained, it builds a model which can fairly accurately predict these language patterns, so when the model is run
The model has high probabilities for these english patterns of text, so the English whakatauki has a much higher probability than the Maori one which lacks most of the english patterns.
